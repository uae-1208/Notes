* P2
  * ReLU   
  * 隐藏单元       输入层    隐藏层   输出层
* P3
  * 监督学习
  * 标准NN       图像-CNN      语音、文本(时间序列数据)-RNN
  * structured data；unstructured data(图像、语音、文本)
* P4
  * SVM是一种传统的机器学习算法，无法处理海量数据
  * 海量数据的产生使神经网络发挥更优良的作用，也越来越受欢迎。训练样本越大，神经网络规模越大，算法的性能越佳。   
  * 深度学习兴起的三大要素：数据、硬件性能、算法创新   
* p7
  * 二元分类     输出值y只能为0/1
  * 特征向量    n(特征向量维度)
  * m  训练样本个数
  * 训练集  测试集
* p8
  * w,b    需要学习的参数  
  * sigmoid()  σ()
* p9
  * 损失函数L(y^，y):描述的是单个训练样本的表现    
  * 不能用误差平方，因为梯度下降法无法使其优化
  * 成本函数J(ω,b):描述的是在某一参数(ω,b)下整体训练集的表现
* p10
  * 凸函数    梯度下降法
  * α学习率:梯度下降法的步长
* p13
  * 计算图   dj/dv=dv
* p15  
  * ω₁、ω₂是参数向量ω中的两个元素，x₁和x₂是特征向量x中的两个元素
* p16
  * m样本下dω也是平均值
* p17
  * SIMD运算   单指令多数据    GPU和CPU都有，但GPU性能更好
  * 向量化可以使用python内置的函数，这样可以调用硬件的并行数据流操作，因此比for循环运算更快
* p21
  * python中的广播
* p22
  * reshape
  * rand(n)声明的是一个长度为n的数组，而不是维度为n×1的列向量，在神经网络中要避免使用它，而是声明确定维度的矩阵。
  * python中+-*/都是元素之间的操作，*也是，不能理解为矩阵的乘法
* p24
  * p(yⁱ|xⁱ)
  * 极大似然估计不太懂
* p26
    * 隐藏层:在训练中，这些中间数据无法被直接观察到，就像被隐藏起来了
    * 激活:不同层之间传递的值
    * 输入层被定义为第零层，且计算神经网络的层数时不考虑它
    * 每一层的激活和参数左上角都有方框角标，其中第i层激活的角标是i-1，参数的角标是i
* p30
    * 激活函数的选择中，tanh()要比σ()好，因为它使激活的平均值更接近0，这可以让下一层的学习更方便一些(后面会讲原因)    
    * tanh()几乎在任何场景下都更适合充当激活函数，除了在输出层当中
    * tanh()和σ()都有的缺点是当输入很大或者很小时，导数接近于0，这拖慢了梯度下降的速度
    * 使用ReLU作为激活函数也是一种不错的选择，它的导数始终为1(输入＞0时)，可以让学习速度很快
    * 泄露ReLU
* p31
    * 不使用激活函数或者使用线性的激活函数，输出只能是输入的线性组合加上偏移
    * 在回归问题中可以使用线性的激活函数，但是一般只能用于输出层中

  * 回归问题:预测输入值和输出值之间的关系，拟合出输入到输出之间映射函数

  * 步骤:
    * ①确定合适的模型
    * ②导入数据集(要完成标定，归一化)
    * ③选择学习算法(梯度下降)
    * ④测试结果(测试集)       
  * 特征值可能差异较大，可能对测试结果的影响权重不同，所以要归一化
  * logistic回归(逻辑回归)也是一种线性回归，主要用于解决多分类问题
* p32
    * 求解激活函数的导数时，尽量表示成激活函数的组合关系，方便计算，如 σ'=σ(1-σ)
* p33
    * 两层神经网络中，最后一张ppt左边为单样本下的后向求导，右边为m个训练样本的后向求导的伪代码
* p34
    * 初始化参数时同一隐藏层的每个结点参数不能相同，否则每个结点都会进行相同的计算，这样是没有意义的
    * 初始化参数时一般参数都设置得比较小，因为tanh()和σ()在z=0时导数比较大，学习速度快
* p37
    * 多层神经网络前向传递和后向传递中的一般公式
* p39
    *  第l层参数的维度: w=(n^[l],n^[l-1])=dw,    b=(n^[l],1)=z=a=db,    Z=(n[l],m)=A=dZ=dA
    * 多利用assert来断言中间变量的shape，这样可以减少bug
* p40
    * 多隐藏层神经网络之所以表现得更好可以理解为，每一隐藏层所承担的任务由浅入深。如图像检测中，第一层的作用是检测一些边缘特征，第二层的作用是检测一些器官如鼻子、嘴巴，第三层的作用是检测整体面部。在CNN中这种解释会更加直观，敬请期待。
    * 一般来说，要想实现与隐藏层数多的神经网络模型相同的效果，浅层神经网络的隐藏层节点数要呈指数增长。
    * 深度学习实际上就是隐藏层数多的深层神经网络。
* p42
    * hyper parameters:学习率、迭代次数、隐藏层个数、每个隐藏层的节点数、每个隐藏层的激活函数
* p47
    * 验证集(development set)用于验证不同算法在训练集上的效果，找出最适合的算法模型。用来调参。        
      * 测试数据集只能用一次，用来测试最终的效果。
    * 尽量保证训练集、验证集和测试集来自同一分布，否则训练效果可能不太好。如手机拍的猫图和百度上下载的高清猫图就不属于同一分布。
    * 验证集过拟合到了测试集？  无偏估计？
* p48
  * 训练集误差   偏差   欠拟合(更线性化)
  * 验证集误差   方差   过拟合(更非线性)
  * 李沐的模型容量和数据复杂度二维表
  * 贝叶斯误差:可以理解为数据集标定的误差率，如肉眼鉴别猫图的正确率
* p49
  * 高偏差(不断调整直至偏差降到可接受范围):
    * ①调整隐藏层和隐藏节点个数  
    * ②换模型       
    * ③花费更多时间训练   
    * ④使用优化算法(后续讲)
  * 高方差:
    * ①扩充数据集   
    * ②正则化    
    * ③或者换模型
  * 偏差方差权衡:机器学习发展初期偏差和方差之间存在一定的耦合性，在改进网络时，二者往往此消彼长。而在目前的机器学习中，采取更大的模型并且选择更大的数量集可以降低二者耦合性，可以改善其一而另一指标变化很小
* p50
    * 正则化:我的理解是通过一些手段降低网络复杂度，从而改善过拟合现象
  * L2正则化:成本函数J增加了一项，所以参数w[l]的导数也增加了一项λw[l]/m(λ是超参数)。
  * 范数
  * 权重衰减:w = (1-α·λ/m)w - α·(back pop)
* p51:
    * 增加网络的深度和节点时可以改进欠拟合，但是会增加过拟合的可能性
    * 由于权重衰减，学习完成的w会变得比正则化之前更小，减小了一些隐藏节点的影响，简化了复杂网络，抑制了过拟合
* p52
    * Dropout:随机地删除一些隐藏节点，每个样本和每次的迭代中删除情况都不一样
    * Keep_Prob:每一隐藏层保留节点的概率
    * 正向传播和反向传播要除以Keep_Prob，因为要保证数据的期望不变。
* p53
    * Dropout的合理性:对于一个隐藏节点来说，当输入不同的样本以及在不同迭代过程中，它的每一个输入都有可能被随机地抹除，因此它对任一输入都不能过分信任(即设置较大的权重)，如此起到简化网络的作用(与L2正则化同理)。
    * Dropout的缺点是在不同的迭代中，成本函数J的定义不再明确。
    * 只需在训练中使用正则化，无需在验证和测试中使用。     
      * 因为使用正则化的目的是训练出理想的参数w和b，而测试时不应该修改模型。
* p54
    * 通过增加数据复杂度来抑制过拟合:
      * ①扩充新的数据集
      * ②数据增广
    * early stopping:缺点是无法充分优化J
* p55
    * 标准化:X的每个维度的μ=0，σ²=1   这与之前说的归一化不太一样
    * 如果X的不同维度的数据范围差异很大(未经过标准化)，J的图形可能比较狭长，而标准化后的J的图形可能更圆润。    
      * 圆润的J可以让我们选择更大的学习率(在狭长图形中大学习率让参数看起来反复横跳，不容易收敛)，从而更快收敛，提高训练速度。
* p56
    * 梯度爆炸/消失:在深层网络中，由于隐藏层的不断累加会导致各个层的w和b的偏导特别大或者特别小。      
      * w的各个元素要接近1才不会发生梯度消失/梯度爆炸。
    * 梯度爆炸:迭代初期学习率必须比较小，否则参数会在收敛值周围反复横跳。但是迭代后期减小的学习率会使训练时间大大变长。
    * 梯度消失:迭代初期学习率必须比较大，否则训练速度会特别慢。但是迭代后期减大的学习率会使训练得不到收敛。
* p57
    * 初始化参数时不再在randn后乘上0.01，而是:
      * ①tanh激活函数:np.sqrt(1/n)
      * ②relu激活函数:np.sqrt(2/n)          
        * 如此一定程度上可缓和梯度消失/梯度爆炸，但不理解为啥
* p58
    * 近似求导时双边差分的准确度要比单边差分高，因为前者关于ε的等价无穷小要更高阶。因此在使用梯度检验时，一般使用双标差分。
* p59
    * 当ε取10⁻⁷时，检验视频中的式子是否比10⁻⁷小。如若不然，依次查询每一层w和b的每个元素，观察dθ与dθapprox是否差距过大，差距较大者很有可能就是bug所在
* p60
    * 不需要在训练时使用梯度检验，在debug或者代码编写完成后的检查中使用即可
    * 如果使用了L2正则化(或其它修正J的正则化)，在梯度检验中也要使用该正则化
    * 在Dropout中不能使用梯度检验
* P61
    * mini batch子集
    * epoch
* P62
    * J是曲折下降的
    * 一般的梯度下降是full batch
    * 在一轮epoch中，mini batch的梯度下降次数(也就是子集个数)比full batch多，所以训练速度更快。不过mini batch并不是每次的梯度下降中J都是递减的，因为一轮epoch中不同梯度下降所遍历的数据子集不同。
    * mini batch中训练one epoch所消耗的时间实际上比迭代一次普通的梯度下降更长，因为vectorization的优势没有得到充分发挥。mini batch之所以可以加速训练是因为每次计算batch都可以让梯度下降一次，训练one epoch后梯度下降的次数就是整个数据集中batch的个数。
* P63
    * 指数加权(移动)平均
    * Vt = β*Vt-1 + (1-β)*θt          
    * β接近于1时，Vt可近似看成1/(1-β)个过往θ的平均值：证明见P65
* P64
    * 指数加权(移动)平均可近似实现1/(1-β)个过往θ的平均效果，这种方法代码更简短且跟节省存储空间(不然如果要计算n个数的平均值，还需要将这n个值全都存储下来)
* P65
    * 偏移修正:修正指数加权平均的前期估计不准确
* P66
    * V_dW = β*V_dW + (1-β)*dW     V_db = β*V_db + (1-β)*db  
    * 动量梯度下降主要用来抑制梯度仿佛横跳的现象，使优化路径少走了一些弯路
* P67
    * RMSprop:让大的更大，小的更小
* P68
    * Momentum和RMSprop缝合
    * β₁=0.9   β₂ = 0.999     ε=10⁻⁸
* P69
    * decay rate
    * 还有好几种衰减公式手动修改(适用于训练时间长的情况)
    * decay rate的优先级可以低一些
* P70
    * 当模型比较复杂时，J是一个高维函数(参数w和b数量很多)，此时函数J存在局部最优的可能性很小，而是存在很多的鞍点。
    * 鞍点周围一般都比较平稳，参数训练时一般都要经过很长的平稳区间(即使如此，也已经是下降最快的路径了)到达鞍点后，才慢慢脱离平稳区间加速下降。      
      * Momentum、RmsRrop、Adam这种算法可以加速这个过程(why？)
* P71
    * 超参数：
      * α     
      * #iteration        
      * activation_funs       
      * #hidden_lays       
      * #hidden_units      
      * keep_pro                     
      * mini_batch_size       
      * β  β₁  β₂     
      * decay_rate
    * 优先级:  
      * 1.α       
      * 2.#hidden_units/mini_batch_size/β       
      * 3.#hidden_lays/mini_batch_size
    * 最好通过随机的当时选取超参数。
    * 先找出模型表现比较好的超参数大致范围，然后在这个小范围内更加细致地调参。
* P72
    * 将参数划分为不同的分段，然后在每个分段内投入均匀的计算资源来检索比较合适的参数。
    * 不要线性地划分分段，而是指数划分。         
      * 这是因为参数在不同区间的敏感度不一样，应该将更多的不敏感的区间划分成同一个分段，否则会浪费计算资源。如指数加权平均中，β为0.9时和在0.995时增加0.001，所引起的效果变化是十分不同的（前者始终用大约10个过往值取平均 ，后者取过往值的个数从200个变成了250个）。
* P73
    * 有时候一个模型运行几个月后，由于一些微小的变化(如收集到的数据不同了)原来的超参数不再适用，需要我们更新超参数。
    * 不同领域中，调整超参数的经验可能是无法移植的。
    * Pandas和Cavia两种调参方法的选择取决于计算资源的多少。
* P74
    * 归一化某一层是为了下一层的参数更好训练（不太能完全理解）
    * batch normalization:在每一层中，首先对上一层的激活值(该层的输入值)进行归一化，再使用激活函数。
    * γ和β         
      * 两种新的参数，非超参数。
* P75
    * 使用batch norm可以不用再管参数b，因为输入值减去它的期望时，b抵消掉了(7min30s)。
* P76
    * 在学习过程中，参数w和b会不断变化，导致隐藏层的输入(上一层的激活值)在不同的迭代过程中也会变化，而batch norm使每一层的输入的期望为β，方差为γ，让其不会偏移过大。这增强了隐藏层对前面层的w和b的变化适应性，降低了层间的耦合，使层与层之间更加独立，提升学习速率。     
    * bn还会有一些轻微的正则化效果（7min50s）。
* P77
    * 在训练中，对每次的μ和σ进行指数加权平均，然后将得到的值用于测试中。因为测试时每次使用的是单样本，无法计算μ和σ。
* P78
    * Sofamax回归是Logistic回归从二分类到多分类的扩展
    * Sofamax回归中的输出层：Z→t→a             
      * a使多种输出的概论总和为1
    * Sofamax回归的损失函数定义与Logistic回归不一样

